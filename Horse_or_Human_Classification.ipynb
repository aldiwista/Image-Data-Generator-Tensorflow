{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Horse-or-Human-Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwZBlgFAqDyp",
        "outputId": "e3ae3b41-e36f-4132-d121-5e2f81d7ff52"
      },
      "source": [
        "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip -O /tmp/horse-or-human.zip\r\n",
        "!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip  -O /tmp/validation-horse-or-human.zip"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-02-18 10:37:03--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 108.177.127.128, 2a00:1450:4013:c01::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 149574867 (143M) [application/zip]\n",
            "Saving to: ‘/tmp/horse-or-human.zip’\n",
            "\n",
            "/tmp/horse-or-human 100%[===================>] 142.65M  54.7MB/s    in 2.6s    \n",
            "\n",
            "2021-02-18 10:37:06 (54.7 MB/s) - ‘/tmp/horse-or-human.zip’ saved [149574867/149574867]\n",
            "\n",
            "--2021-02-18 10:37:06--  https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 108.177.126.128, 108.177.127.128, 2a00:1450:4013:c01::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|108.177.126.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 11480187 (11M) [application/zip]\n",
            "Saving to: ‘/tmp/validation-horse-or-human.zip’\n",
            "\n",
            "/tmp/validation-hor 100%[===================>]  10.95M  35.0MB/s    in 0.3s    \n",
            "\n",
            "2021-02-18 10:37:07 (35.0 MB/s) - ‘/tmp/validation-horse-or-human.zip’ saved [11480187/11480187]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2-S7gF9sx_u"
      },
      "source": [
        "import os\r\n",
        "import zipfile\r\n",
        "\r\n",
        "local_zip = '/tmp/horse-or-human.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('/tmp/horse-or-human')\r\n",
        "local_zip = '/tmp/validation-horse-or-human.zip'\r\n",
        "zip_ref = zipfile.ZipFile(local_zip, 'r')\r\n",
        "zip_ref.extractall('/tmp/validation-horse-or-human')\r\n",
        "zip_ref.close()"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xdb-kSbMu_p4",
        "outputId": "b14dfe65-3924-4694-8131-83179fc4feb2"
      },
      "source": [
        "train_horse_dir = os.path.join('/tmp/horse-or-human/horses')\r\n",
        "train_human_dir = os.path.join('/tmp/horse-or-human/humans')\r\n",
        "\r\n",
        "validation_horse_dir = os.path.join('/tmp/validation-horse-or-human/horses')\r\n",
        "validation_human_dir = os.path.join('/tmp/validation-horse-or-human/humans')\r\n",
        "\r\n",
        "train_horses_names = os.listdir('/tmp/horse-or-human/horses')\r\n",
        "print(train_horses_names[:10])\r\n",
        "train_human_names = os.listdir('/tmp/horse-or-human/humans')\r\n",
        "print(train_human_names[:10])\r\n",
        "\r\n",
        "validation_horse_names = os.listdir('/tmp/validation-horse-or-human/horses')\r\n",
        "print(validation_horse_names[:10])\r\n",
        "\r\n",
        "validation_human_names = os.listdir('/tmp/validation-horse-or-human/humans')\r\n",
        "print(validation_human_names[:10])\r\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['horse23-1.png', 'horse35-9.png', 'horse43-5.png', 'horse49-9.png', 'horse08-5.png', 'horse33-0.png', 'horse35-2.png', 'horse10-1.png', 'horse17-9.png', 'horse10-3.png']\n",
            "['human14-09.png', 'human13-28.png', 'human11-04.png', 'human05-16.png', 'human03-08.png', 'human09-06.png', 'human15-21.png', 'human12-01.png', 'human06-28.png', 'human05-09.png']\n",
            "['horse5-402.png', 'horse1-335.png', 'horse4-541.png', 'horse1-241.png', 'horse1-105.png', 'horse5-303.png', 'horse4-188.png', 'horse1-539.png', 'horse4-548.png', 'horse5-504.png']\n",
            "['valhuman04-09.png', 'valhuman05-21.png', 'valhuman03-17.png', 'valhuman03-13.png', 'valhuman01-17.png', 'valhuman03-00.png', 'valhuman02-22.png', 'valhuman02-10.png', 'valhuman01-01.png', 'valhuman02-08.png']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KPOvNXtqe65"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lx9QZ0EPz1za",
        "outputId": "1edc7cc3-35cf-4912-e754-294069659a24"
      },
      "source": [
        "model = tf.keras.models.Sequential([\r\n",
        "    tf.keras.layers.Conv2D(16, (3,3,), activation='relu', input_shape=(300, 300, 3)),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),\r\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\r\n",
        "    tf.keras.layers.Flatten(),\r\n",
        "    tf.keras.layers.Dense(512, activation='relu'),\r\n",
        "    tf.keras.layers.Dense(128, activation='relu'),\r\n",
        "\r\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\r\n",
        "])\r\n",
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_6 (Conv2D)            (None, 298, 298, 16)      448       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2 (None, 149, 149, 16)      0         \n",
            "_________________________________________________________________\n",
            "conv2d_7 (Conv2D)            (None, 147, 147, 32)      4640      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_6 (MaxPooling2 (None, 73, 73, 32)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_8 (Conv2D)            (None, 71, 71, 64)        18496     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_7 (MaxPooling2 (None, 35, 35, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_9 (Conv2D)            (None, 33, 33, 64)        36928     \n",
            "_________________________________________________________________\n",
            "max_pooling2d_8 (MaxPooling2 (None, 16, 16, 64)        0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 16384)             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               8389120   \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 128)               65664     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 1)                 129       \n",
            "=================================================================\n",
            "Total params: 8,515,425\n",
            "Trainable params: 8,515,425\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PsWJ9wLE2HN3"
      },
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=0.001),\r\n",
        "              loss='binary_crossentropy',\r\n",
        "              metrics=['acc'])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4s2c9X5K3aFE",
        "outputId": "4a593c22-2808-4370-f5b2-b89d90966fe5"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\r\n",
        "\r\n",
        "train_datagen = ImageDataGenerator(\r\n",
        "    rescale=1./255,\r\n",
        ")\r\n",
        "\r\n",
        "train_generator = train_datagen.flow_from_directory(\r\n",
        "    '/tmp/horse-or-human',\r\n",
        "    target_size = (300, 300),\r\n",
        "    batch_size =128,\r\n",
        "    class_mode='binary'\r\n",
        ")\r\n",
        "\r\n",
        "validation_datagen = ImageDataGenerator(\r\n",
        "    rescale= 1./255\r\n",
        ")\r\n",
        "\r\n",
        "validation_generator = validation_datagen.flow_from_directory(\r\n",
        "    '/tmp/validation-horse-or-human', target_size = (300,300), class_mode = 'binary'\r\n",
        ")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 1027 images belonging to 2 classes.\n",
            "Found 256 images belonging to 2 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Jm-sZ9l4Nmz",
        "outputId": "97f8941b-3b8e-4875-8424-7cd54358b1bf"
      },
      "source": [
        "history = model.fit(train_generator, steps_per_epoch =8, epochs = 15, verbose=1, validation_data=validation_generator)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "8/8 [==============================] - 92s 10s/step - loss: 6.5910 - acc: 0.5920 - val_loss: 0.6198 - val_acc: 0.5273\n",
            "Epoch 2/15\n",
            "8/8 [==============================] - 76s 9s/step - loss: 0.6623 - acc: 0.6268 - val_loss: 5.0976 - val_acc: 0.5000\n",
            "Epoch 3/15\n",
            "8/8 [==============================] - 77s 9s/step - loss: 1.7743 - acc: 0.6280 - val_loss: 1.0930 - val_acc: 0.5156\n",
            "Epoch 4/15\n",
            "8/8 [==============================] - 77s 9s/step - loss: 0.6826 - acc: 0.7395 - val_loss: 1.9687 - val_acc: 0.5625\n",
            "Epoch 5/15\n",
            "8/8 [==============================] - 77s 9s/step - loss: 0.4622 - acc: 0.7646 - val_loss: 0.3200 - val_acc: 0.8711\n",
            "Epoch 6/15\n",
            "8/8 [==============================] - 77s 10s/step - loss: 0.3516 - acc: 0.8242 - val_loss: 1.2949 - val_acc: 0.6484\n",
            "Epoch 7/15\n",
            "8/8 [==============================] - 77s 10s/step - loss: 0.3246 - acc: 0.8442 - val_loss: 0.4805 - val_acc: 0.8906\n",
            "Epoch 8/15\n",
            "8/8 [==============================] - 96s 12s/step - loss: 0.2351 - acc: 0.9088 - val_loss: 0.8579 - val_acc: 0.8828\n",
            "Epoch 9/15\n",
            "8/8 [==============================] - 78s 10s/step - loss: 0.2769 - acc: 0.8571 - val_loss: 0.8379 - val_acc: 0.8164\n",
            "Epoch 10/15\n",
            "8/8 [==============================] - 77s 10s/step - loss: 0.1792 - acc: 0.9340 - val_loss: 1.3660 - val_acc: 0.8008\n",
            "Epoch 11/15\n",
            "8/8 [==============================] - 85s 11s/step - loss: 0.0720 - acc: 0.9738 - val_loss: 1.2687 - val_acc: 0.8633\n",
            "Epoch 12/15\n",
            "8/8 [==============================] - 78s 10s/step - loss: 0.0599 - acc: 0.9749 - val_loss: 1.5761 - val_acc: 0.8633\n",
            "Epoch 13/15\n",
            "8/8 [==============================] - 86s 11s/step - loss: 0.0138 - acc: 0.9944 - val_loss: 0.7081 - val_acc: 0.9102\n",
            "Epoch 14/15\n",
            "8/8 [==============================] - 76s 9s/step - loss: 0.4594 - acc: 0.8700 - val_loss: 0.7776 - val_acc: 0.8555\n",
            "Epoch 15/15\n",
            "8/8 [==============================] - 76s 9s/step - loss: 0.0582 - acc: 0.9784 - val_loss: 2.0351 - val_acc: 0.7734\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGcNq8Rw4i-Y"
      },
      "source": [
        " "
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}